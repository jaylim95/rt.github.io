[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI for Pancreas Cancer",
    "section": "",
    "text": "Background\nIn this dataset, collected at Indiana University School of Medicine, the researchers gathered a series of biomarkers from the urine of three groups of patients:\nThe age and sex of the patients are also recorded. The goal was to develop an accurate way to identify patients with pancreatic cancer.",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "AI for Pancreas Cancer",
    "section": "",
    "text": "Healthy controls\nPatients with non-cancerous pancreatic conditions, like chronic pancreatitis\nPatients with pancreatic ductal adenocarcinoma",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "index.html#the-4-urinary-biomarkers",
    "href": "index.html#the-4-urinary-biomarkers",
    "title": "AI for Pancreas Cancer",
    "section": "The 4 Urinary Biomarkers",
    "text": "The 4 Urinary Biomarkers\nThe 4 Urinary Biomarkers collected and its description is as follow:\n\nCreatinine is a protein that is often used as an indicator of kidney function.\nYVLE1 is lymphatic vessel endothelial hyaluronan receptor 1, a protein that may play a role in tumor metastasis\nREG1B is a protein that may be associated with pancreas regeneration\nTFF1 is trefoil factor 1, which may be related to regeneration and repair of the urinary tract",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "index.html#task",
    "href": "index.html#task",
    "title": "AI for Pancreas Cancer",
    "section": "Task",
    "text": "Task\nOur goal is to build an AI algorithm that can predict patients that has pancreas cancers and those who don’t, as accurately as possible.\nLet’s start by exploring the dataset to understand the task better!\n\\(\\,\\)",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "01_Section_1.html",
    "href": "01_Section_1.html",
    "title": "1  Understanding the Data",
    "section": "",
    "text": "1.1 Overview\nBefore we begin our analysis on the pancreatic cancer dataset, it is essential to thoroughly understand the dataset. This includes examining the dataset’s structure, understanding the predictors, identifying any missing values, and recognizing the significance of each column. This foundational understanding will help us make informed decisions during our analysis and ensure the accuracy and reliability of our findings.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Understanding the Data</span>"
    ]
  },
  {
    "objectID": "01_Section_1.html#a-look-at-the-dataset",
    "href": "01_Section_1.html#a-look-at-the-dataset",
    "title": "1  Understanding the Data",
    "section": "1.2 A look at the Dataset",
    "text": "1.2 A look at the Dataset\nThe dataset contains 590 rows and 10 columns. Each row represents a unique patient, and each column provides specific information about the patient’s diagnosis, treatment, and other relevant factors.\nHere’s the dataset:\n\n\n\n\n\n\n\n\n\n\n\n\nWhat’s our target variable?\n\n\n\n\n\nOur Target Variable is the diagnosis column. Diagnosis = 0 indicates patient that does not have cancer and diagnosis = 1 indicates that patient is diagnosed with Pancreatic ductal adenocarcinoma (PDAC).\n\n\n\n\n1.2.1 Columns\nThere are 10 columns in the dataset, namely\n\nPatient Cohort : Details which patients cohort a particular patient is from. In our case, there are 2 cohorts, Cohort 1 and Cohort 2\nAge : Details the age of the patient\nSex : Details the gender of the patient. F stands for Female, M stands for maLE\nDiagnosis : Details the diagnosis of the patient. 1 stands for diagnosed with Pancreatic ductal adenocarcinoma (PDAC) and 0 stands for not-diagnosed with cancer.\nplasma CA19_9 : Details the amount of a protein called CA 19-9 in the blood, which is often used in monitoring pancreatic cancer\nCreatinine : Details the level of creatinine of the patient, which is a protein that is often used as an indicator of kidney function.\nYVLE1 : Details the level of YVLE1, a urinaly biomarker, which may play a role in tumor metastasis\nREG1A : Details the level of REG1A, which may be associated with pancreas regeneration\nREG1B : Details the level of REG1B, a urinary biomarker, which may be associated with pancreas regeneration\nTFF1 : Details the level of trefoil factor 1, which may be related to regeneration and repair of the urinary tract\n\n\n\n\n\n\n\nCan you attempt to just look at the data and spot some relationship?\n\n\n\nIn the next section, we will visualize the columns to test whethere there are any observable patterns or interesting trends that might give insight before building our AI model.\n\\(\\,\\)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Understanding the Data</span>"
    ]
  },
  {
    "objectID": "02_Section_2.html",
    "href": "02_Section_2.html",
    "title": "2  Visualizing the Data",
    "section": "",
    "text": "2.1 Visualization\nFill description.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing the Data</span>"
    ]
  },
  {
    "objectID": "02_Section_2.html#univariate-categorial-variables",
    "href": "02_Section_2.html#univariate-categorial-variables",
    "title": "2  Visualizing the Data",
    "section": "2.2 Univariate: Categorial Variables",
    "text": "2.2 Univariate: Categorial Variables\nFill description.\n\n\n\n\n\n\nSelect a categorical column to visualize:\n\ndata = FileAttachment(\"cleaned_dataset.csv\").csv({ typed: true })\ncat_columns = ['patient_cohort', 'diagnosis', 'sex']\nviewof selected_column_cat = Inputs.radio(cat_columns, {value: cat_columns[0]})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngrouped_data = d3.groups(data, d =&gt; d[selected_column_cat]).map(([key, values]) =&gt; ({key, count: values.length}));\n\nPlot.plot({\n  marks: [\n    Plot.barY(grouped_data, {x: \"key\", y: \"count\", fill: \"steelblue\"}),\n    Plot.text(grouped_data, {x: \"key\", y: \"count\", text: d =&gt; d.count, dy: -10})\n  ],\n  x: {\n    label: selected_column_cat\n  },\n  y: {\n    label: \"Count\"\n  },\n  color: {\n    legend: false\n  }\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2.1 What did you observe?\n\n\n\n\n\n\nWhat did you observe about patient cohort and sex?\n\n\n\n\n\nWe see that they are approximately equal amount of female and males, and even though cohort 1 has more patients than cohort 2, there are approximately similar!\n\n\n\n\n\n\n\n\n\nWhat did you observe about diagnosis (target variable)?\n\n\n\n\n\nWe see that around 400 patients who does not have cancers, while only approximately 180 patients have cancers! That implies only approximately 30% of our data is pancreatic cancer patients!\nBrain teaser: What impact it might have? \nHint: Think about evalaution metrics, and whether it is enough information to train a classifiers!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing the Data</span>"
    ]
  },
  {
    "objectID": "02_Section_2.html#univariate-continuous-variables",
    "href": "02_Section_2.html#univariate-continuous-variables",
    "title": "2  Visualizing the Data",
    "section": "2.3 Univariate: Continuous Variables",
    "text": "2.3 Univariate: Continuous Variables\nFill description.\n\n\n\n\n\n\nSelect a continuous column to visualize:\n\ncont_columns = ['age', 'LYVE1', 'REG1B', 'TFF1', 'REG1A', 'creatinine', 'plasma_CA19_9']\nviewof selected_column = Inputs.radio(cont_columns, {value: 'LYVE1'})\nviewof bin_count = Inputs.range([1, 50], {step: 1, value: 15, label: \"Number of Bins\"})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  marks: [\n    Plot.rectY(data, Plot.binX({y: \"count\"}, {x: selected_column, fill: \"steelblue\", thresholds: bin_count}))\n  ],\n  x: {\n    label: selected_column\n  },\n  y: {\n    label: \"Count\"\n  },\n  color: {\n    legend: false\n  }\n})\n\n\n\n\n\n\nFill thoughts.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing the Data</span>"
    ]
  },
  {
    "objectID": "02_Section_2.html#bivariate-categorial-variables",
    "href": "02_Section_2.html#bivariate-categorial-variables",
    "title": "2  Visualizing the Data",
    "section": "2.4 Bivariate: Categorial Variables",
    "text": "2.4 Bivariate: Categorial Variables\nFill description.\n\n\n\n\n\n\nSelect a column to compare against diagnosis:\n\nbivariate_cat_columns = ['sex', 'patient_cohort']\nviewof selected_bivariate_cat_column = Inputs.radio(bivariate_cat_columns, {value: bivariate_cat_columns[0]})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunction calculateGroupedPercentages(data, groupColumn, stackColumn) {\n  const grouped = d3.rollup(data, v =&gt; v.length, d =&gt; d[groupColumn], d =&gt; d[stackColumn]);\n  const totals = d3.rollup(data, v =&gt; v.length, d =&gt; d[groupColumn]);\n\n  return Array.from(grouped, ([key, values]) =&gt; {\n    const total = totals.get(key);\n    return Array.from(values, ([stackKey, count]) =&gt; ({\n      group: key,\n      stack: stackKey,\n      count,\n      percentage: (count / total) * 100\n    }));\n  }).flat();\n}\n\ngrouped_bivariate_data = calculateGroupedPercentages(data, \"diagnosis\", selected_bivariate_cat_column);\n\nPlot.plot({\n  marks: [\n    Plot.barY(grouped_bivariate_data, {\n      x: d =&gt; d.group + \":\" + d.stack,\n      y: \"percentage\",\n      fill: \"stack\",\n      title: d =&gt; `${d.stack}: ${d.percentage.toFixed(1)}%`\n    }),\n    Plot.text(grouped_bivariate_data, {\n      x: d =&gt; d.group + \":\" + d.stack,\n      y: d =&gt; d.percentage,\n      text: d =&gt; `${d.percentage.toFixed(1)}%`,\n      dy: -4\n    })\n  ],\n  x: {\n    label: \"Diagnosis\",\n    domain: Array.from(new Set(grouped_bivariate_data.map(d =&gt; d.group + \":\" + d.stack))),\n    tickFormat: d =&gt; d.split(\":\")[0] // Format ticks to show only the group\n  },\n  y: {\n    label: \"Percentage\"\n  },\n  color: {\n    legend: true\n  }\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.4.1 What did you observe?\nFill thoughts.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing the Data</span>"
    ]
  },
  {
    "objectID": "02_Section_2.html#bivariate-continuous-variables",
    "href": "02_Section_2.html#bivariate-continuous-variables",
    "title": "2  Visualizing the Data",
    "section": "2.5 Bivariate: Continuous Variables",
    "text": "2.5 Bivariate: Continuous Variables\nFill description about heatmap.\n\n\n\n\n\n\nSelect Continuous Variables for Correlation Heatmap:\n\nbivariate_cont_columns = ['age', 'LYVE1', 'REG1B', 'TFF1', 'REG1A', 'creatinine', 'plasma_CA19_9']\nviewof selected_bivariate_cont_columns = Inputs.checkbox(bivariate_cont_columns, {value: bivariate_cont_columns})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunction calculateCorrelationMatrix(data, selectedColumns) {\n  const n = selectedColumns.length;\n  const correlationMatrix = Array.from({ length: n }, () =&gt; Array(n).fill(0));\n\n  function pearsonCorrelation(x, y) {\n    const meanX = d3.mean(x);\n    const meanY = d3.mean(y);\n    const diffX = x.map(d =&gt; d - meanX);\n    const diffY = y.map(d =&gt; d - meanY);\n    const numerator = d3.sum(diffX.map((d, i) =&gt; d * diffY[i]));\n    const denominator = Math.sqrt(d3.sum(diffX.map(d =&gt; d * d)) * d3.sum(diffY.map(d =&gt; d * d)));\n    return numerator / denominator;\n  }\n\n  for (let i = 0; i &lt; n; i++) {\n    for (let j = 0; j &lt; n; j++) {\n      const col1 = selectedColumns[i];\n      const col2 = selectedColumns[j];\n      const values1 = data.map(d =&gt; d[col1]);\n      const values2 = data.map(d =&gt; d[col2]);\n      const correlation = pearsonCorrelation(values1, values2);\n      correlationMatrix[i][j] = correlation;\n    }\n  }\n\n  return correlationMatrix;\n}\n\ncorrelation_matrix = calculateCorrelationMatrix(data, selected_bivariate_cont_columns);\ncorrelation_data = selected_bivariate_cont_columns.flatMap((col1, i) =&gt;\n  selected_bivariate_cont_columns.map((col2, j) =&gt; ({\n    x: col1,\n    y: col2,\n    value: correlation_matrix[i][j]\n  }))\n);\n\nPlot.plot({\n  marks: [\n    Plot.cell(correlation_data, {x: \"x\", y: \"y\", fill: \"value\", title: d =&gt; d.value.toFixed(2)}),\n    Plot.text(correlation_data, {x: \"x\", y: \"y\", text: d =&gt; d.value.toFixed(2), dy: 0, textAnchor: \"middle\"})\n  ],\n  x: {\n    domain: selected_bivariate_cont_columns,\n    label: \"Variables\"\n  },\n  y: {\n    domain: selected_bivariate_cont_columns,\n    label: \"Variables\"\n  },\n  color: {\n    type: \"linear\",\n    scheme: \"blues\",\n    label: \"Correlation\"\n  },\n  width: 600,\n  height: 600\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.5.1 Try to match it on a scatterplot!\n\n\n\n\n\n\nSelect a X-Axis and Y-Axis Column:\n\nviewof x_column = Inputs.radio(bivariate_cont_columns, {value: 'age'})\nviewof y_column = Inputs.radio(bivariate_cont_columns, {value: 'LYVE1'})\nPlot.plot({\n  marks: [\n    Plot.dot(data, {x: x_column, y: y_column, fill: \"steelblue\"})\n  ],\n  x: {\n    label: x_column\n  },\n  y: {\n    label: y_column\n  },\n  color: {\n    legend: false\n  }\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.5.2 What did you observe?\nFill thoughts.\n\\(\\,\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing the Data</span>"
    ]
  },
  {
    "objectID": "03_Section_3.html",
    "href": "03_Section_3.html",
    "title": "3  Baseline",
    "section": "",
    "text": "3.1 Building Baseline\nA baseline model serves as a simple reference point for evaluating the performance of more complex models. It represents the minimum level of performance that any model should achieve. Common baselines include predicting the most frequent class or random guessing.\nEstablishing a baseline is crucial because it provides a benchmark to compare against. It helps in understanding whether the added complexity of a model is actually improving performance. If a model does not perform better than the baseline, it indicates that the model might not be useful.\nThis is some starter code to build the baseline models. Do not change the following code. Just run it! :)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Baseline</span>"
    ]
  },
  {
    "objectID": "03_Section_3.html#building-baseline",
    "href": "03_Section_3.html#building-baseline",
    "title": "3  Baseline",
    "section": "",
    "text": "Please enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Baseline</span>"
    ]
  },
  {
    "objectID": "03_Section_3.html#understanding-the-metrics",
    "href": "03_Section_3.html#understanding-the-metrics",
    "title": "3  Baseline",
    "section": "3.2 Understanding the metrics",
    "text": "3.2 Understanding the metrics\nWe are going to investigate 4 metrics to evalaute the performance of our machine learning algorithms, namely, accuracy, precision, recall and F1 score. We will go through it one-by-one in the following:\n\n3.2.1 Accuracy\nThe ratio of correctly predicted instances to the total instances. Mathematically it is defined as \\[\\text{Accuracy} =\\frac{TP+TN}{\\text{Total}}\\]\n\n\n\n\n\n\nWhy can’t we blindly trust accuracy?\n\n\n\n\n\nIn our dataset, 30% of the people are diagnosed with pancreatic cancer, and 70% are not. If we rely only on accuracy, our model could just predict everyone as not having cancer and still be 70% accurate. This doesn’t help us identify those who actually have cancer. That’s why we need to look at other metrics like precision and recall to ensure our model is correctly identifying both those with and without the disease.\n\n\n\n\n\n3.2.2 Precision\nBefore we move on to talk about precision, recall & F1 score, we would like you to think about the following problem:\n\n\n\n\n\n\nWhat is the impact of false positive and false negative in diagnosing pancreas cancer?\n\n\n\n\n\nFalse Positive: The patient doesn’t have pancreatic cancer, but the AI incorrectly diagnoses them with it. This can lead to unnecessary tests and procedures, causing stress and potential harm.\nFalse Negative: The patient has pancreatic cancer, but the AI incorrectly diagnoses them as not having it. This is very dangerous as the patient misses the chance for early treatment, leading to potentially severe consequences.\n\n\n\nThus, precision, recall and F1 score are important metrics to help us to get a hollistic picture of our model performance.\nPrecision is defined as the ratio of true positive predictions to the total predicted positives. Mathematically, \\[\\text{Precision} =\\frac{TP}{TP+FP}\\]\nPrecision tells us how many of the patients predicted to have PDAC actually have the disease. High precision means fewer false positives.\n\n\n3.2.3 Recall\nRecall is defined as the ratio of true positive predictions to the total actual positives. Mathematically, it is defined as\n\\[\\text{Recall} = \\frac{TP}{TP+FN}\\]\nRecall (or sensitivity) indicates how many patients with PDAC are correctly identified by the model. High recall means fewer false negatives, which is crucial for medical diagnosis.\n\n\n3.2.4 F1 Score\nF1 Score is the harmonic mean of precision and recall. Mathematically, it is defined as\n\\[\\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\\]\nThe F1 Score provides a balance between precision and recall. It is particularly useful when the class distribution is imbalanced, as it considers both false positives and false negatives.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Baseline</span>"
    ]
  },
  {
    "objectID": "03_Section_3.html#baseline-1-all-false",
    "href": "03_Section_3.html#baseline-1-all-false",
    "title": "3  Baseline",
    "section": "3.3 Baseline 1: All False",
    "text": "3.3 Baseline 1: All False\nFirst, we will create a baseline around the most frequent class, which in our case, is 0, where we assume all patients do not have cnacer.This baseline mimics a current system where we assume that all patients do not have Pancreatic ductal adenocarcinoma (PDAC).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nWhy is precision, recall and F1 score are all 0 in our case?\n\n\n\n\n\nPrecision, recall, and F1 score are all zero because the baseline model predicts all patients as negative (no PDAC), resulting in no true positives!\nDid you realize the importance of different metrics in judging performance?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Baseline</span>"
    ]
  },
  {
    "objectID": "03_Section_3.html#baseline-2-random-guess",
    "href": "03_Section_3.html#baseline-2-random-guess",
    "title": "3  Baseline",
    "section": "3.4 Baseline 2: Random Guess",
    "text": "3.4 Baseline 2: Random Guess\nRandom guessing is a baseline method where predictions are made by randomly assigning classes (positive or negative) without any consideration of the input features. As an analogy, it is like you flipping a coin and randomly assign a patient of its status based on the coins (for example, has cancer if the coin is head, and has no cancer if the coin is tail).\nRandom guessing sets a very low bar for model performance. If a machine learning model cannot outperform random guessing, it indicates that the model has not captured any meaningful patterns from the data. Beating random guessing demonstrates that the model has predictive power and can provide reliable results.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\\(\\,\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Baseline</span>"
    ]
  },
  {
    "objectID": "04_Section_4.html",
    "href": "04_Section_4.html",
    "title": "4  Logistic Regression",
    "section": "",
    "text": "4.1 Logistic Regression\nFill description.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "04_Section_4.html#logistic-regression",
    "href": "04_Section_4.html#logistic-regression",
    "title": "4  Logistic Regression",
    "section": "",
    "text": "4.1.1 Quiz : Draw a line!",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "04_Section_4.html#train-test-split",
    "href": "04_Section_4.html#train-test-split",
    "title": "4  Logistic Regression",
    "section": "4.2 Train-Test Split",
    "text": "4.2 Train-Test Split\nFill description.\nFirst, we will perform a train-test split.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "04_Section_4.html#algorithm",
    "href": "04_Section_4.html#algorithm",
    "title": "4  Logistic Regression",
    "section": "4.3 Algorithm",
    "text": "4.3 Algorithm\nNext, we will run logistic regression on our training dataset, then compute the performance of the metrics using our dataset and then\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nCan you try to delete the class weight and changing the C-value to observe what happened to the performance of the algorithm?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "04_Section_4.html#feature-importance",
    "href": "04_Section_4.html#feature-importance",
    "title": "4  Logistic Regression",
    "section": "4.4 Feature Importance",
    "text": "4.4 Feature Importance\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\\(\\,\\)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "05_Section_5.html",
    "href": "05_Section_5.html",
    "title": "5  Decision Tree",
    "section": "",
    "text": "5.1 Decision Tree\nFill description.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Decision Tree</span>"
    ]
  },
  {
    "objectID": "05_Section_5.html#train-test-split",
    "href": "05_Section_5.html#train-test-split",
    "title": "5  Decision Tree",
    "section": "5.2 Train-Test Split",
    "text": "5.2 Train-Test Split\nAs we have done previosuly under logistic regression, we will first perform a train-test split.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Decision Tree</span>"
    ]
  },
  {
    "objectID": "05_Section_5.html#algorithm",
    "href": "05_Section_5.html#algorithm",
    "title": "5  Decision Tree",
    "section": "5.3 Algorithm",
    "text": "5.3 Algorithm\nNext, we will run decision treeon our training dataset, then compute the performance of the metrics using our dataset and then\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nTry Removing Class Weight, how does the metrics changes?\n\n\n\n\n\nAccuracy stays the same at 0.89. However, we observe the precision increased and recall decreased. The F1 score stays approximately the same.\nWhy Precision Improved?\nPrecision increases when the class weight is removed, suggesting that the model makes fewer false positive predictions. This might be because, without class balancing, the model is biased towards the majority class.\nWhy Recall Decreased?\nRecall decreases when the class weight is removed, indicating that the model misses more true positive instances. This is critical in medical diagnoses where missing a positive case (false negative) can be detrimental.\n\n\n\n\n\n\n\n\n\nAssume class_weight=\"balanced\", first test with max_depth=None, and then test with max_depth=1, observe the difference in the performance, and also in visualization of the tree?\n\n\n\n\n\nReducing max_depth from None to 1 decreases the recall and F1 score, indicating the model becomes less capable of identifying true positives, likely due to oversimplification. The oversimplication is reflected in the visualization of the tree.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Decision Tree</span>"
    ]
  },
  {
    "objectID": "05_Section_5.html#visualizing-the-tree",
    "href": "05_Section_5.html#visualizing-the-tree",
    "title": "5  Decision Tree",
    "section": "5.4 Visualizing the tree",
    "text": "5.4 Visualizing the tree\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\\(\\,\\)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Decision Tree</span>"
    ]
  }
]